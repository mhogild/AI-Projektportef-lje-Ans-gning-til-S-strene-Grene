from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import OpenAIEmbeddings
from langchain_community.chat_models import ChatOpenAI
from langchain.chains import RetrievalQA
import os

# ğŸ” SÃ¦t din OpenAI API-nÃ¸gle (du mÃ¥ gerne sÃ¦tte denne via miljÃ¸variabel i stedet)
os.environ["OPENAI_API_KEY"] = "XX" #Skjult - Du skal vÃ¦re velkommen til at spÃ¸rge efter API hvis du gerne vil teste den :)

# 1. Load gemte embeddings fra Chroma
vectorstore = Chroma(
    persist_directory="./chroma_test1",
    embedding_function=OpenAIEmbeddings()
)

# 2. Retriever
retriever = vectorstore.as_retriever(search_kwargs={"k": 3})

# 3. GPT-model og kÃ¦de
llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0.5)

qa = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=retriever,
    return_source_documents=True
)

# 4. Chat loop
while True:
    query = input("\nğŸ” Stil et spÃ¸rgsmÃ¥l om Ã¥rsrapporten (eller skriv 'exit'): ")
    if query.lower() in ["exit", "quit"]:
        break

    result = qa.invoke(query)
    print("\nğŸ’¬ Svar:\n", result['result'])

    print("\nğŸ“„ Kilder:")
    for doc in result['source_documents']:
        print(f" - {doc.metadata.get('source')} | ...{doc.page_content[:100]}...\n")
